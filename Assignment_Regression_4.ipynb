{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b8eb5e",
   "metadata": {},
   "source": [
    "#### Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d84a3",
   "metadata": {},
   "source": [
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that incorporates regularization to improve the model's generalization and interpretability\n",
    "\n",
    "Differences from Other Regression:\n",
    "\n",
    "Ridge Regression: Ridge regression also applies regularization but uses an L2 penalty, which is the sum of the squares of the coefficients. The L2 penalty in Ridge regression shrinks the coefficients but does not set any of them exactly to zero. This means Ridge regression tends to retain all features, though with smaller weights, rather than performing feature selection like Lasso.\n",
    "\n",
    "Elastic Net Regression: Elastic Net is a hybrid of Lasso and Ridge regression. It combines both L1 and L2 penalties in its loss function. Elastic Net is particularly useful when there are many correlated features. While Lasso may arbitrarily select one feature from a group of correlated features, Elastic Net can include or exclude groups of correlated features, providing a balance between feature selection and coefficient shrinkage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6aceb6",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8998dc57",
   "metadata": {},
   "source": [
    "#### Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4452ead4",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection is its ability to automatically perform variable selection by shrinking some coefficients to zero. This is due to the L1 regularization penalty applied in Lasso Regression, which promotes sparsity in the model coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1d582",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b875e711",
   "metadata": {},
   "source": [
    "#### Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390036a8",
   "metadata": {},
   "source": [
    "\n",
    "In Lasso Regression, the coefficients indicate the strength and direction of the relationship between each feature and the target variable:\n",
    "\n",
    "- Non-zero Coefficients: Represent features that have a significant impact on the target variable. A larger absolute value means a stronger effect.\n",
    "- Zero Coefficients: Indicate that the corresponding features are excluded from the model, meaning they are not important or redundant due to multicollinearity.\n",
    "- Effect of lambda (Regularization Parameter): A higher lambda value increases regularization strength, leading to more coefficients being set to zero (feature selection) and a sparser model.\n",
    "- Importance of Standardization: Features should be standardized before fitting the model to ensure coefficients are comparable and correctly interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6d61a",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf1b7e1",
   "metadata": {},
   "source": [
    "#### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b42177",
   "metadata": {},
   "source": [
    "In Lasso Regression, the primary tuning parameter that can be adjusted is the regularization parameter lambda. Adjusting lambda affects the modelâ€™s complexity and performance, but there are also other considerations, like feature scaling.\n",
    "- Higher lambda : Reduces model complexity by selecting fewer features (sparse model), improves interpretability, reduces variance but may increase bias (risk of underfitting).\n",
    "- Lower lambda: Retains more features (dense model), can capture more complexity in data, reduces bias but may increase variance (risk of overfitting).\n",
    "- Feature Scaling: Ensures fair penalty application across features, leading to more reliable coefficient shrinkage and feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7fb620",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75dcbf",
   "metadata": {},
   "source": [
    "#### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624db427",
   "metadata": {},
   "source": [
    "Lasso Regression is inherently a linear model, meaning it fits a linear relationship between the features and the target variable. However, Lasso can be adapted for non-linear regression problems by transforming the input features or using methods that allow for non-linearity.\n",
    "- Polynomial and Interaction Terms: Add polynomial features to model non-linear relationships, then apply Lasso for feature selection.\n",
    "- Basis Functions and Splines: Use basis functions (e.g., sine, Gaussian) or splines to capture non-linear patterns and apply Lasso to the transformed data.\n",
    "- Kernel Methods: Use kernels to map data into higher dimensions, making non-linear patterns linear.\n",
    "- Transformations of the Target Variable: Apply transformations to linearize relationships before using Lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b14f95",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428d29dc",
   "metadata": {},
   "source": [
    "#### Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb89a0f",
   "metadata": {},
   "source": [
    "\n",
    "Ridge Regression and Lasso Regression are both linear regression techniques that use regularization to prevent overfitting by penalizing large coefficients. However, they differ in how they apply regularization, which affects their feature selection abilities and model interpretation.\n",
    "\n",
    "Ridge Regression:\n",
    "- Uses an L2 penalty (squared coefficients).\n",
    "- Shrinks all coefficients but keeps them non-zero.\n",
    "- Does not perform feature selection; keeps all features.\n",
    "- Better for handling multicollinearity among features.\n",
    "\n",
    "Lasso Regression:\n",
    "- Uses an L1 penalty (absolute values of coefficients).\n",
    "- Shrinks some coefficients to exactly zero, effectively performing feature selection.\n",
    "- Produces a sparser model with fewer features.\n",
    "- Useful when only a few features are expected to be important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc59c54",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06df5681",
   "metadata": {},
   "source": [
    "#### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05fa3dc",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can handle multicollinearity in the input features, but in a different way compared to Ridge Regression:\n",
    "How Lasso Handles Multicollinearity\n",
    "Feature Selection by Shrinking Coefficients to Zero:\n",
    "\n",
    "Lasso Regression uses an L1 penalty, which encourages sparsity in the model by shrinking some coefficients to exactly zero. When features are highly correlated (multicollinearity), Lasso may choose one feature from a group of correlated features and set the coefficients of the others to zero. This effectively reduces the redundancy caused by multicollinearity by selecting only the most predictive feature(s) from a set of correlated features.\n",
    "Simplicity and Interpretability:\n",
    "\n",
    "By setting some coefficients to zero, Lasso Regression simplifies the model, making it easier to interpret and reducing overfitting. This can be particularly useful when we want to identify the most important features in the presence of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7220b7c",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6143a9fc",
   "metadata": {},
   "source": [
    "#### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d549f60",
   "metadata": {},
   "source": [
    "Choosing the optimal value of the regularization parameter lambda in Lasso Regression is crucial for balancing model complexity and performance. Here are common methods to find the optimal lambda:\n",
    "\n",
    "1. Cross-Validation\n",
    "K-Fold Cross-Validation: Split the dataset into K folds. Train the Lasso model on K-1 folds and validate on the remaining fold. Repeat this process K times, each time with a different fold as the validation set. Evaluate the model's performance using metrics like Mean Squared Error (MSE) or R-squared.\n",
    "Selecting lambda: For each candidate lambda, compute the average performance across all folds. Choose the lambda that minimizes the cross-validated error.\n",
    "2. Validation Set\n",
    "Train-Validation Split: Divide the dataset into a training set and a validation set. Train the Lasso model on the training set for different values of lambda and evaluate performance on the validation set. Select the lambda with the best performance.\n",
    "3. Regularization Path Algorithms\n",
    "Coordinate Descent: Use algorithms that compute the entire regularization path, i.e., the solution for a range of lambda values. This approach helps to find the optimal lambda efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc0f34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
