{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10dc4e0",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd31e180",
   "metadata": {},
   "source": [
    "#### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8bd387",
   "metadata": {},
   "source": [
    "### R-squared in Linear Regression\n",
    "\n",
    "R-squared is a statistical measure that represents the proportion of the variance in the dependent variable (the variable we're trying to predict) that is explained by the independent variables (the predictors) in a linear regression model. It provides an indication of how well the independent variables explain the variability in the dependent variable.\n",
    "\n",
    "R-squared is calculated as:\n",
    "\n",
    "R-squared = 1 - SSR/SST\n",
    "\n",
    "where:\n",
    "\n",
    "SSR = Sum Of Suare Resediuls\n",
    "SST = Total Sum of Square"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116decff",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd212c8",
   "metadata": {},
   "source": [
    "#### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b572e8",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of the R-squared statistic that adjusts for the number of predictors in the model. It is designed to provide a more accurate measure of how well the model explains the variability in the dependent variable, especially when multiple predictors are involved.\n",
    "\n",
    "R-squared measures the proportion of variance explained by the model but increases with the addition of any predictor, regardless of its relevance.\n",
    "\n",
    "Adjusted R-squared also measures the proportion of variance explained but adjusts for the number of predictors, penalizing the inclusion of unnecessary variables, and thus better reflects model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628e7d27",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ebfd9",
   "metadata": {},
   "source": [
    "#### Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d47e6",
   "metadata": {},
   "source": [
    "Use adjusted R-squared when we have multiple predictors or when comparing models with different numbers of predictors. It helps avoid overfitting by penalizing unnecessary variables, offering a more accurate measure of model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802b6d9a",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2c4610",
   "metadata": {},
   "source": [
    "#### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c866b00",
   "metadata": {},
   "source": [
    "MSE (Mean Squared Error): Measures the average of the squared differences between actual and predicted values. It penalizes larger errors more heavily.\n",
    "\n",
    "RMSE (Root Mean Squared Error): The square root of MSE, giving an error metric in the same units as the target variable. It's sensitive to large errors.\n",
    "\n",
    "MAE (Mean Absolute Error): Measures the average of the absolute differences between actual and predicted values. It's less sensitive to large errors compared to MSE and RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d998b14",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4673116",
   "metadata": {},
   "source": [
    "#### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fb336",
   "metadata": {},
   "source": [
    "Mean Squared Error (MSE):\n",
    "-Advantages:\n",
    "  - Penalizes larger errors more heavily, which can be useful if we want to focus on reducing big errors.\n",
    "-Disadvantages:\n",
    "  - Sensitive to outliers because errors are squared, which can disproportionately affect the metric.\n",
    "  - Less interpretable since it’s in squared units of the target variable.\n",
    "\n",
    "Root Mean Squared Error (RMSE):\n",
    "-Advantages:\n",
    "  - Provides an error metric in the same units as the target variable, making it easier to understand and communicate.\n",
    "  - Like MSE, it penalizes large errors, which can be beneficial if large errors are particularly problematic.\n",
    "-Disadvantages:\n",
    "  - Still emphasizes large errors, which might not always be desirable depending on the context.\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "-Advantages:\n",
    "  - Less affected by outliers since it doesn’t square the errors, providing a more balanced view of typical errors.\n",
    "  - Easy to interpret as it is in the same units as the target variable.\n",
    "-Disadvantages:\n",
    "  - Doesn’t differentiate between large and small errors, which can be a limitation if large errors need special attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ba313",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c861ce5",
   "metadata": {},
   "source": [
    "#### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b768eb",
   "metadata": {},
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a type of regression technique that introduces a penalty equivalent to the absolute value of the magnitude of the coefficients.\n",
    "Lasso can shrink some coefficients to exactly zero, effectively selecting a simpler model by including only the most relevant features. This is particularly useful in high-dimensional datasets where some features may be irrelevant.\n",
    "\n",
    "Ridge regularization is another type of regularization that penalizes the sum of the squares of the coefficients.\n",
    "\n",
    "Differences:\n",
    "\n",
    "Lasso:\n",
    "- Uses the absolute values of the coefficients\n",
    "- Can shrink some coefficients to exactly zero, leading to sparse models.\n",
    "Effective for feature selection, especially when dealing with high-dimensional data with irrelevant features\n",
    "\n",
    "Ridge:\n",
    "- Uses the squares of the coefficients\n",
    "- Shrinks coefficients but never zeroes them out completely, leading to models where all features are retained but with smaller coefficients.\n",
    "- Does not perform feature selection as effectively because it doesn’t shrink coefficients to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a8ce1",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e622dab",
   "metadata": {},
   "source": [
    "#### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d6923",
   "metadata": {},
   "source": [
    "Overfitting occurs when a machine learning model learns not only the underlying patterns in the training data but also the noise and fluctuations, leading to poor generalization to unseen data. Regularized linear models help prevent overfitting by adding a penalty term to the loss function, which discourages the model from learning overly complex or extreme weights for the features.\n",
    "Regularization techniques, like Ridge and Lasso, add a penalty to the loss function that grows with the magnitude of the coefficients.\n",
    "\n",
    "Example : Let's consider a scenario where we have a dataset with a polynomial relationship between the features and the target variable:\n",
    "\n",
    "Without Regularization:\n",
    "Suppose we fit a high-degree polynomial regression model to the data. The model may perfectly fit the training data, capturing every small fluctuation, leading to a complex curve with very high coefficients.\n",
    "This model might perform very poorly on new data because it has overfit the noise in the training data.\n",
    "\n",
    "With Regularization:\n",
    "If we apply Ridge or Lasso regularization to the same high-degree polynomial model, the penalty term will discourage the model from assigning large coefficients to the polynomial terms.\n",
    "As a result, the model will be simpler, with smaller coefficients, and will better capture the underlying pattern rather than the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8cf02",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f1e04e",
   "metadata": {},
   "source": [
    "#### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69847f8",
   "metadata": {},
   "source": [
    "Linear Relationship Assumption:\n",
    "\n",
    "These models assume a linear relationship between features and the target, making them unsuitable for capturing complex, non-linear patterns.\n",
    "Bias Introduction:\n",
    "\n",
    "Regularization can introduce bias, potentially leading to underfitting if the model is too simple for the data.\n",
    "Limited Feature Selection (Ridge):\n",
    "\n",
    "Ridge regression doesn't zero out coefficients, so all features are retained, including irrelevant ones, which can reduce interpretability.\n",
    "Sensitivity to Scaling:\n",
    "\n",
    "Regularized models are sensitive to the scale of features, requiring careful preprocessing to ensure proper performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5949e680",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859bc25a",
   "metadata": {},
   "source": [
    "#### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2721b44",
   "metadata": {},
   "source": [
    "RMSE (Root Mean Squared Error): Measures the square root of the average squared differences between predicted and actual values. It gives more weight to larger errors, making it sensitive to outliers.\n",
    "\n",
    "MAE (Mean Absolute Error): Measures the average of the absolute differences between predicted and actual values. It treats all errors equally, providing a more straightforward interpretation.\n",
    "\n",
    "Model B (with an MAE of 8) might be seen as the better performer for consistency in predictions. However, if outliers are a significant concern and need to be minimized, Model A's RMSE of 10 might offer a more reliable measure of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d49d3c",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
